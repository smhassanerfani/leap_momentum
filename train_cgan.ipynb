{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6aeb30e-9a1c-4ea3-8788-eb8908896873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "from dataset import Agulhas\n",
    "from models.CycleGAN import Discriminator, Generator\n",
    "from joint_transforms import Transform\n",
    "\n",
    "from utils import save_checkpoint, load_checkpoint, save_examples, csv_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f302a9e6-2b75-49c7-a77a-30d70f7b0e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CycleGAN/snapshots\n"
     ]
    }
   ],
   "source": [
    "MODEL=\"CycleGAN\"\n",
    "LEARNING_RATE=1.0e-5\n",
    "NUM_EPOCHS=30\n",
    "INPUT_SIZE=256\n",
    "BATCH_SIZE=2\n",
    "NUM_WORKERS=0\n",
    "LAMBDA_CYCLE=10\n",
    "LAMBDA_IDENTITY=0\n",
    "BETAS=[0.5, 0.999]\n",
    "MOMENTUM=0.0\n",
    "WEIGHT_DECAY=0.0\n",
    "POWER=0.0\n",
    "\n",
    "LAMBDA_IDENTITY = 0.0\n",
    "LAMBDA_CYCLE = 10\n",
    "\n",
    "SCRATCH_BUCKET = os.environ['SCRATCH_BUCKET']\n",
    "SNAPSHOT_DIR = os.path.join(MODEL, 'snapshots')\n",
    "print(SNAPSHOT_DIR)\n",
    "\n",
    "USE_CHECKPOINT = False\n",
    "RESTORE_FROM = os.path.join(MODEL, 'snapshots', 'epoch-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c2297eb-705f-4957-b7fb-aac0a787acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(disc_H, disc_Z, gen_Z, gen_H, loader, opt_disc, opt_gen, l1, mse):\n",
    "    \n",
    "    H_reals = 0\n",
    "    H_fakes = 0\n",
    "    \n",
    "    logger = dict()\n",
    "    running_disc_loss = 0.0\n",
    "    running_gen_loss = 0.0\n",
    "    for idx, (zebra, horse) in enumerate(loader):\n",
    "        zebra = zebra.cuda()\n",
    "        horse = horse.cuda()\n",
    "\n",
    "        # Train Discriminators H and Z\n",
    "\n",
    "        fake_horse = gen_H(zebra)\n",
    "        D_H_real = disc_H(horse)\n",
    "        D_H_fake = disc_H(fake_horse.detach())\n",
    "        # H_reals += D_H_real.mean().item()\n",
    "        # H_fakes += D_H_fake.mean().item()\n",
    "        D_H_real_loss = mse(D_H_real, torch.ones_like(D_H_real))\n",
    "        D_H_fake_loss = mse(D_H_fake, torch.zeros_like(D_H_fake))\n",
    "        D_H_loss = D_H_real_loss + D_H_fake_loss\n",
    "\n",
    "        fake_zebra = gen_Z(horse)\n",
    "        D_Z_real = disc_Z(zebra)\n",
    "        D_Z_fake = disc_Z(fake_zebra.detach())\n",
    "        D_Z_real_loss = mse(D_Z_real, torch.ones_like(D_Z_real))\n",
    "        D_Z_fake_loss = mse(D_Z_fake, torch.zeros_like(D_Z_fake))\n",
    "        D_Z_loss = D_Z_real_loss + D_Z_fake_loss\n",
    "\n",
    "        # put it togethor\n",
    "        D_loss = (D_H_loss + D_Z_loss) / 2\n",
    "        \n",
    "        opt_disc.zero_grad()\n",
    "        D_loss.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        # Train Generators H and Z\n",
    "        # adversarial loss for both generators\n",
    "        D_H_fake = disc_H(fake_horse)\n",
    "        D_Z_fake = disc_Z(fake_zebra)\n",
    "        loss_G_H = mse(D_H_fake, torch.ones_like(D_H_fake))\n",
    "        loss_G_Z = mse(D_Z_fake, torch.ones_like(D_Z_fake))\n",
    "\n",
    "        # cycle loss\n",
    "        cycle_zebra = gen_Z(fake_horse)\n",
    "        cycle_horse = gen_H(fake_zebra)\n",
    "        cycle_zebra_loss = l1(zebra, cycle_zebra)\n",
    "        cycle_horse_loss = l1(horse, cycle_horse)\n",
    "\n",
    "        # identity loss (remove these for efficiency if you set lambda_identity=0)\n",
    "        identity_zebra = gen_Z(zebra)\n",
    "        identity_horse = gen_H(horse)\n",
    "        identity_zebra_loss = l1(zebra, identity_zebra)\n",
    "        identity_horse_loss = l1(horse, identity_horse)\n",
    "\n",
    "        # add all togethor\n",
    "        G_loss = (\n",
    "            loss_G_Z\n",
    "            + loss_G_H\n",
    "            + cycle_zebra_loss * LAMBDA_CYCLE\n",
    "            + cycle_horse_loss * LAMBDA_CYCLE\n",
    "            + identity_horse_loss * LAMBDA_IDENTITY\n",
    "            + identity_zebra_loss * LAMBDA_IDENTITY\n",
    "        )\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        G_loss.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        # Statistics\n",
    "        D_loss = D_loss.detach()\n",
    "        G_loss = G_loss.detach()\n",
    "        running_disc_loss += D_loss.item()\n",
    "        running_gen_loss += G_loss.item()\n",
    "        \n",
    "\n",
    "        if idx % 200 == 0:\n",
    "            print(f\"Discriminator Loss: {D_loss.item():.5f}, Generator Loss: {G_loss.item():.5f}\")\n",
    "            \n",
    "    logger['disc_loss'] = running_disc_loss / len(loader)\n",
    "    logger['gen_loss'] = running_gen_loss / len(loader)\n",
    "\n",
    "    return logger\n",
    "\n",
    "def val_loop(dataloader, transform_params, model, saving_path):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for counter, (x, y) in enumerate(dataloader, 1):\n",
    "\n",
    "            # GPU deployment\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            # Compute prediction and loss\n",
    "            y_fake = model(x)\n",
    "\n",
    "            save_examples(x, y, y_fake, transform_params, counter, saving_path)\n",
    "            if counter == 5:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1514b4cb-24ad-40e4-8203-77c4f7282cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    cudnn.enabled = True\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "    print(f\"{MODEL} is deployed on {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(SNAPSHOT_DIR)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    # Loading model\n",
    "    disc_H = Discriminator(in_channels=1).cuda()\n",
    "    disc_Z = Discriminator(in_channels=1).cuda()\n",
    "    gen_Z = Generator(img_channels=1, num_residuals=9).cuda()\n",
    "    gen_H = Generator(img_channels=1, num_residuals=9).cuda()\n",
    "    \n",
    "    opt_disc = optim.Adam(\n",
    "        list(disc_H.parameters()) + list(disc_Z.parameters()),\n",
    "        lr=LEARNING_RATE,\n",
    "        betas=BETAS,\n",
    "    )\n",
    "\n",
    "    opt_gen = optim.Adam(\n",
    "        list(gen_Z.parameters()) + list(gen_H.parameters()),\n",
    "        lr=LEARNING_RATE,\n",
    "        betas=BETAS,\n",
    "    )\n",
    "\n",
    "    L1 = nn.L1Loss()\n",
    "    mse = nn.MSELoss()\n",
    "    \n",
    "    joint_transforms = Transform()\n",
    "    \n",
    "    train_dataset = Agulhas(split='train', joint_transform=joint_transforms)\n",
    "    val_dataset = Agulhas(split='val', joint_transform=joint_transforms)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                  num_workers=NUM_WORKERS, pin_memory=False, drop_last=False)\n",
    "\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=True,\n",
    "                                num_workers=NUM_WORKERS, pin_memory=False, drop_last=False)\n",
    "    \n",
    "    transform_params = dict()\n",
    "    transform_params['inputs_mean'] = train_dataset.inps_mean_std[0]\n",
    "    transform_params['inputs_std'] = train_dataset.inps_mean_std[1]\n",
    "    transform_params['targets_mean'] = train_dataset.tars_mean_std[0]\n",
    "    transform_params['targets_std'] = train_dataset.tars_mean_std[1]\n",
    "\n",
    "    if USE_CHECKPOINT:\n",
    "        load_checkpoint(\n",
    "            f'{RESTORE_FROM}/gen_h.pth',\n",
    "            gen_H,\n",
    "            opt_gen,\n",
    "            LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            f'{RESTORE_FROM}/gen_z.pth',\n",
    "            gen_Z,\n",
    "            opt_gen,\n",
    "            LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            f'{RESTORE_FROM}/disc_h.pth',\n",
    "            disc_H,\n",
    "            opt_disc,\n",
    "            LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            f'{RESTORE_FROM}/disc_z.pth',\n",
    "            disc_Z,\n",
    "            opt_disc,\n",
    "            LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        logger = train_fn(\n",
    "            disc_H,\n",
    "            disc_Z,\n",
    "            gen_Z,\n",
    "            gen_H,\n",
    "            train_dataloader,\n",
    "            opt_disc,\n",
    "            opt_gen,\n",
    "            L1,\n",
    "            mse\n",
    "        )\n",
    "        \n",
    "        logger['epoch'] = epoch\n",
    "        csv_writer(logger, SNAPSHOT_DIR)\n",
    "        \n",
    "        try:\n",
    "            current_epoch_directory = os.path.join(SNAPSHOT_DIR, f'epoch-{epoch:02d}')\n",
    "            os.makedirs(os.path.join(current_epoch_directory, 'Z'))\n",
    "            os.makedirs(os.path.join(current_epoch_directory, 'H'))\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        val_loop(val_dataloader, transform_params, gen_Z, f'{current_epoch_directory}/Z')\n",
    "        val_loop(val_dataloader, transform_params, gen_H, f'{current_epoch_directory}/H') \n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            save_checkpoint(gen_H, opt_gen, filename=os.path.join(current_epoch_directory, \"gen_h.pth\"))\n",
    "            save_checkpoint(gen_Z, opt_gen, filename=os.path.join(current_epoch_directory, \"gen_z.pth\"))\n",
    "            save_checkpoint(disc_H, opt_disc, filename=os.path.join(current_epoch_directory, \"disc_h.pth\"))\n",
    "            save_checkpoint(disc_Z, opt_disc, filename=os.path.join(current_epoch_directory, \"disc_z.pth\"))\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c8d4fd5-0a04-4b74-a8a0-43951f6f2d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CycleGAN is deployed on Tesla T4\n",
      "Discriminator Loss: 0.52753, Generator Loss: 17.37900\n",
      "Discriminator Loss: 0.35851, Generator Loss: 12.86691\n",
      "Discriminator Loss: 0.22450, Generator Loss: 6.39027\n",
      "Discriminator Loss: 0.20615, Generator Loss: 5.70717\n",
      "Discriminator Loss: 0.26843, Generator Loss: 13.51129\n",
      "Discriminator Loss: 0.15081, Generator Loss: 5.61358\n",
      "Discriminator Loss: 0.07458, Generator Loss: 5.54880\n",
      "Discriminator Loss: 0.10078, Generator Loss: 6.13331\n",
      "Saving Checkpoint...\n",
      "Saving Checkpoint...\n",
      "Saving Checkpoint...\n",
      "Saving Checkpoint...\n",
      "Discriminator Loss: 0.14689, Generator Loss: 7.61846\n",
      "Discriminator Loss: 0.11928, Generator Loss: 4.76870\n",
      "Discriminator Loss: 0.04901, Generator Loss: 3.30104\n",
      "Discriminator Loss: 0.09374, Generator Loss: 9.10740\n",
      "Discriminator Loss: 0.04731, Generator Loss: 7.80068\n",
      "Discriminator Loss: 0.09533, Generator Loss: 9.12804\n",
      "Discriminator Loss: 0.06687, Generator Loss: 7.44008\n",
      "Discriminator Loss: 0.05881, Generator Loss: 5.96836\n",
      "Discriminator Loss: 0.05224, Generator Loss: 3.70094\n",
      "Discriminator Loss: 0.08147, Generator Loss: 8.62800\n",
      "Discriminator Loss: 0.02720, Generator Loss: 6.05480\n",
      "Discriminator Loss: 0.04641, Generator Loss: 4.10960\n",
      "Discriminator Loss: 0.02663, Generator Loss: 5.25243\n",
      "Discriminator Loss: 0.04752, Generator Loss: 5.03509\n",
      "Discriminator Loss: 0.02340, Generator Loss: 3.15844\n",
      "Discriminator Loss: 0.02668, Generator Loss: 5.52702\n",
      "Discriminator Loss: 0.04580, Generator Loss: 3.35666\n",
      "Discriminator Loss: 0.04033, Generator Loss: 7.30171\n",
      "Discriminator Loss: 0.05675, Generator Loss: 7.84680\n",
      "Discriminator Loss: 0.08752, Generator Loss: 8.25364\n",
      "Discriminator Loss: 0.03243, Generator Loss: 5.85406\n",
      "Discriminator Loss: 0.03272, Generator Loss: 7.03243\n",
      "Discriminator Loss: 0.01292, Generator Loss: 5.47809\n",
      "Discriminator Loss: 0.06175, Generator Loss: 4.50875\n",
      "Discriminator Loss: 0.09869, Generator Loss: 5.78539\n",
      "Discriminator Loss: 0.03360, Generator Loss: 3.46846\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 81\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m     load_checkpoint(\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRESTORE_FROM\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/disc_z.pth\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     74\u001b[0m         disc_Z,\n\u001b[1;32m     75\u001b[0m         opt_disc,\n\u001b[1;32m     76\u001b[0m         LEARNING_RATE,\n\u001b[1;32m     77\u001b[0m     )\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[0;32m---> 81\u001b[0m     logger \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisc_H\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisc_Z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgen_Z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgen_H\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopt_disc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopt_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mL1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmse\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     logger[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m epoch\n\u001b[1;32m     94\u001b[0m     csv_writer(logger, SNAPSHOT_DIR)\n",
      "Cell \u001b[0;32mIn[3], line 74\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(disc_H, disc_Z, gen_Z, gen_H, loader, opt_disc, opt_gen, l1, mse)\u001b[0m\n\u001b[1;32m     72\u001b[0m D_loss \u001b[38;5;241m=\u001b[39m D_loss\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     73\u001b[0m G_loss \u001b[38;5;241m=\u001b[39m G_loss\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m---> 74\u001b[0m running_disc_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mD_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m running_gen_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m G_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce96eb-35fd-4b3f-a0ca-0781de2a20f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
