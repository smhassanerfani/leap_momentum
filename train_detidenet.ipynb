{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "145406cd-a382-4e7a-8c79-f3d43e1504e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "from dataset import Agulhas2\n",
    "from models.DetideNet import IGWResNet\n",
    "from joint_transforms import Transform2\n",
    "\n",
    "from utils.Pix2Pix import train_loop, val_loop\n",
    "from utils import save_checkpoint, load_checkpoint, csv_writer, save_examples2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcb3cd18-a16a-4942-a2e7-d286df1ba507",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/DetideNet/snapshots\n"
     ]
    }
   ],
   "source": [
    "MODEL=\"DetideNet\"\n",
    "LEARNING_RATE=1.0E-3\n",
    "NUM_EPOCHS=401\n",
    "INPUT_SIZE=256\n",
    "BATCH_SIZE=32\n",
    "NUM_WORKERS=0\n",
    "\n",
    "SCRATCH_BUCKET = os.environ['SCRATCH_BUCKET']\n",
    "SNAPSHOT_DIR = os.path.join('outputs', MODEL, 'snapshots')\n",
    "print(SNAPSHOT_DIR)\n",
    "\n",
    "USE_CHECKPOINT = False\n",
    "RESTORE_FROM = os.path.join(\"DetideNet\", 'snapshots', 'epoch-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "712e6d26-88d7-4a42-90f7-1d4a34a47a43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "loss_l1 = nn.L1Loss()\n",
    "\n",
    "def sshtoqSS(ssh_tensor):\n",
    "    #print(ssh_tensor.shape)\n",
    "    laplacian_x = torch.cuda.FloatTensor([1, -2, 1]).view([1, 1, 1, 3])\n",
    "    laplacian_y = torch.transpose(laplacian_x,2,3)\n",
    "    laplaciansshx = F.conv2d(ssh_tensor, laplacian_x, padding=0)\n",
    "    laplaciansshy = F.conv2d(ssh_tensor, laplacian_y, padding=0)\n",
    "    zp1 = nn.ZeroPad2d((1,1,0,0))\n",
    "    zp2 = nn.ZeroPad2d((0,0,1,1))\n",
    "    return zp1(laplaciansshx) + zp2(laplaciansshy)\n",
    "\n",
    "def lossNN(ytrue, ypred, weight):\n",
    "    mseSSH = loss_fn(ytrue,ypred)\n",
    "    maePV = loss_l1(sshtoqSS(ytrue),sshtoqSS(ypred))\n",
    "    return mseSSH+weight*maePV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eb68661-f969-4508-a0f3-1fa9fbfaf49c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def val_loop(dataloader, transform_params, model, saving_path):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for counter, (ssh, it, bm) in enumerate(dataloader, 1):\n",
    "\n",
    "            # GPU deployment\n",
    "            ssh = ssh.cuda()\n",
    "            it = it.cuda()\n",
    "            bm = bm.cuda()\n",
    "\n",
    "            # Compute prediction and loss\n",
    "            bm_fake = model(ssh)\n",
    "            it_fake = ssh - bm_fake\n",
    "            \n",
    "            y_fake = torch.cat([it_fake, bm_fake], dim=1)\n",
    "            y = torch.cat([it, bm], dim=1)\n",
    "\n",
    "            save_examples2(ssh, y, y_fake, transform_params, counter, saving_path)\n",
    "            if counter == 5:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc2a974-fc5b-4244-90b0-b3228f40dc29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Without xbatcher\n",
    "def main():\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    cudnn.enabled = True\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "    print(f\"{MODEL} is deployed on {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Loading model\n",
    "    model = IGWResNet().cuda()\n",
    "\n",
    "    try:\n",
    "        os.makedirs(SNAPSHOT_DIR)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    # Dataloader\n",
    "    \n",
    "    joint_transforms = Transform2(crop=96)\n",
    "    \n",
    "    train_dataset = Agulhas2(split='train', joint_transform=joint_transforms)\n",
    "    val_dataset = Agulhas2(split='val', joint_transform=joint_transforms)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                  num_workers=NUM_WORKERS, pin_memory=True, drop_last=False)\n",
    "\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=True,\n",
    "                                num_workers=NUM_WORKERS, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # Initializing the loss function and optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "    \n",
    "    \n",
    "    if USE_CHECKPOINT:\n",
    "        load_checkpoint(f'{RESTORE_FROM}/model.pth.tar', model, optimizer, LEARNING_RATE)\n",
    "    \n",
    "    transform_params = dict()\n",
    "    transform_params['inputs_mean'] = train_dataset.inps_mean_std[0]\n",
    "    transform_params['inputs_std'] = train_dataset.inps_mean_std[1]\n",
    "    transform_params['targets_mean'] = train_dataset.tars_mean_std[0]\n",
    "    transform_params['targets_std'] = train_dataset.tars_mean_std[1]\n",
    "    transform_params['targets_bm_mean'] = train_dataset.tars_bm_mean_std[0]\n",
    "    transform_params['targets_bm_std'] = train_dataset.tars_bm_mean_std[1]\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
    "        \n",
    "        train_loss_per_epoch = []\n",
    "        running_loss = 0\n",
    "        for batch_idx, (ssh, _, bm) in enumerate(train_dataloader):\n",
    "\n",
    "            ssh = ssh.to(device)\n",
    "            bm = bm.to(device)\n",
    "\n",
    "            pred = model(ssh)\n",
    "            loss = lossNN(bm, pred, 1.0E3)    \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            oss = loss.detach()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 30 == 0:\n",
    "                print(f\"Epoch: [{epoch}/{NUM_EPOCHS}] Batch: {batch_idx:>2}/{len(train_dataloader)} \"\n",
    "                      f\"Loss: {loss.item():.4f},\")# LR:{lr:#.4E}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss_per_epoch.append(running_loss / len(train_dataloader))\n",
    "        current_dir = os.path.join(SNAPSHOT_DIR, f'epoch-{epoch:003d}')\n",
    "        \n",
    "        try:\n",
    "            os.makedirs(current_dir)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        val_loop(val_dataloader, transform_params, model, current_dir)\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            save_checkpoint(model, optimizer,  os.path.join(current_dir, \"model.pth.tar\"))\n",
    "    \n",
    "    \n",
    "    with open(os.path.join(current_dir, \"loss.npy\"), mode = 'wb') as f:\n",
    "        np.save(f, np.array(train_loss_per_epoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea616e-014b-4550-998f-c03e45dfd7d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetideNet is deployed on Tesla T4\n",
      "Epoch: 0 LR: [0.001]\n",
      "Epoch: [0/201] Batch:  0/241 Loss: 3.3031,\n",
      "Epoch: [0/201] Batch: 30/241 Loss: 3.7344,\n",
      "Epoch: [0/201] Batch: 60/241 Loss: 1.6212,\n",
      "Epoch: [0/201] Batch: 90/241 Loss: 2.1468,\n",
      "Epoch: [0/201] Batch: 120/241 Loss: 2.3538,\n",
      "Epoch: [0/201] Batch: 150/241 Loss: 1.6899,\n",
      "Epoch: [0/201] Batch: 180/241 Loss: 2.5755,\n",
      "Epoch: [0/201] Batch: 210/241 Loss: 2.3188,\n",
      "Epoch: [0/201] Batch: 240/241 Loss: 2.1224,\n",
      "Saving Checkpoint...\n",
      "Epoch: 1 LR: [0.001]\n",
      "Epoch: [1/201] Batch:  0/241 Loss: 1.4065,\n",
      "Epoch: [1/201] Batch: 30/241 Loss: 1.7729,\n",
      "Epoch: [1/201] Batch: 60/241 Loss: 1.8270,\n",
      "Epoch: [1/201] Batch: 90/241 Loss: 2.3510,\n",
      "Epoch: [1/201] Batch: 120/241 Loss: 1.6949,\n",
      "Epoch: [1/201] Batch: 150/241 Loss: 1.8422,\n",
      "Epoch: [1/201] Batch: 180/241 Loss: 2.1323,\n",
      "Epoch: [1/201] Batch: 210/241 Loss: 2.0958,\n",
      "Epoch: [1/201] Batch: 240/241 Loss: 1.2629,\n",
      "Epoch: 2 LR: [0.001]\n",
      "Epoch: [2/201] Batch:  0/241 Loss: 2.1188,\n",
      "Epoch: [2/201] Batch: 30/241 Loss: 2.0681,\n",
      "Epoch: [2/201] Batch: 60/241 Loss: 2.3298,\n",
      "Epoch: [2/201] Batch: 90/241 Loss: 2.2637,\n",
      "Epoch: [2/201] Batch: 120/241 Loss: 1.8555,\n",
      "Epoch: [2/201] Batch: 150/241 Loss: 1.5208,\n",
      "Epoch: [2/201] Batch: 180/241 Loss: 1.4860,\n",
      "Epoch: [2/201] Batch: 210/241 Loss: 1.6488,\n",
      "Epoch: [2/201] Batch: 240/241 Loss: 1.8282,\n",
      "Epoch: 3 LR: [0.001]\n",
      "Epoch: [3/201] Batch:  0/241 Loss: 1.7572,\n",
      "Epoch: [3/201] Batch: 30/241 Loss: 2.3541,\n",
      "Epoch: [3/201] Batch: 60/241 Loss: 1.8486,\n",
      "Epoch: [3/201] Batch: 90/241 Loss: 2.4737,\n",
      "Epoch: [3/201] Batch: 120/241 Loss: 1.8703,\n",
      "Epoch: [3/201] Batch: 150/241 Loss: 1.8109,\n",
      "Epoch: [3/201] Batch: 180/241 Loss: 2.4340,\n",
      "Epoch: [3/201] Batch: 210/241 Loss: 1.6757,\n",
      "Epoch: [3/201] Batch: 240/241 Loss: 1.8418,\n",
      "Epoch: 4 LR: [0.001]\n",
      "Epoch: [4/201] Batch:  0/241 Loss: 1.9631,\n",
      "Epoch: [4/201] Batch: 30/241 Loss: 2.6156,\n",
      "Epoch: [4/201] Batch: 60/241 Loss: 2.3387,\n",
      "Epoch: [4/201] Batch: 90/241 Loss: 1.6230,\n",
      "Epoch: [4/201] Batch: 120/241 Loss: 2.2317,\n",
      "Epoch: [4/201] Batch: 150/241 Loss: 2.3465,\n",
      "Epoch: [4/201] Batch: 180/241 Loss: 1.5244,\n",
      "Epoch: [4/201] Batch: 210/241 Loss: 2.3094,\n",
      "Epoch: [4/201] Batch: 240/241 Loss: 2.0725,\n",
      "Epoch: 5 LR: [0.001]\n",
      "Epoch: [5/201] Batch:  0/241 Loss: 1.8019,\n",
      "Epoch: [5/201] Batch: 30/241 Loss: 2.0267,\n",
      "Epoch: [5/201] Batch: 60/241 Loss: 2.0395,\n",
      "Epoch: [5/201] Batch: 90/241 Loss: 2.0477,\n",
      "Epoch: [5/201] Batch: 120/241 Loss: 1.5730,\n",
      "Epoch: [5/201] Batch: 150/241 Loss: 2.2713,\n",
      "Epoch: [5/201] Batch: 180/241 Loss: 1.8064,\n",
      "Epoch: [5/201] Batch: 210/241 Loss: 1.8247,\n",
      "Epoch: [5/201] Batch: 240/241 Loss: 1.9286,\n",
      "Epoch: 6 LR: [0.001]\n",
      "Epoch: [6/201] Batch:  0/241 Loss: 2.4080,\n",
      "Epoch: [6/201] Batch: 30/241 Loss: 2.3857,\n",
      "Epoch: [6/201] Batch: 60/241 Loss: 2.6324,\n",
      "Epoch: [6/201] Batch: 90/241 Loss: 2.2026,\n",
      "Epoch: [6/201] Batch: 120/241 Loss: 1.6667,\n",
      "Epoch: [6/201] Batch: 150/241 Loss: 1.9797,\n",
      "Epoch: [6/201] Batch: 180/241 Loss: 1.5890,\n",
      "Epoch: [6/201] Batch: 210/241 Loss: 1.9378,\n",
      "Epoch: [6/201] Batch: 240/241 Loss: 2.6721,\n",
      "Epoch: 7 LR: [0.001]\n",
      "Epoch: [7/201] Batch:  0/241 Loss: 2.7053,\n",
      "Epoch: [7/201] Batch: 30/241 Loss: 1.9610,\n",
      "Epoch: [7/201] Batch: 60/241 Loss: 1.7172,\n",
      "Epoch: [7/201] Batch: 90/241 Loss: 1.3241,\n",
      "Epoch: [7/201] Batch: 120/241 Loss: 1.6808,\n",
      "Epoch: [7/201] Batch: 150/241 Loss: 1.9841,\n",
      "Epoch: [7/201] Batch: 180/241 Loss: 2.7875,\n",
      "Epoch: [7/201] Batch: 210/241 Loss: 2.1462,\n",
      "Epoch: [7/201] Batch: 240/241 Loss: 2.1398,\n",
      "Epoch: 8 LR: [0.001]\n",
      "Epoch: [8/201] Batch:  0/241 Loss: 3.1051,\n",
      "Epoch: [8/201] Batch: 30/241 Loss: 1.6518,\n",
      "Epoch: [8/201] Batch: 60/241 Loss: 2.7688,\n",
      "Epoch: [8/201] Batch: 90/241 Loss: 2.4253,\n",
      "Epoch: [8/201] Batch: 120/241 Loss: 1.3607,\n",
      "Epoch: [8/201] Batch: 150/241 Loss: 1.6943,\n",
      "Epoch: [8/201] Batch: 180/241 Loss: 2.2267,\n",
      "Epoch: [8/201] Batch: 210/241 Loss: 1.5397,\n",
      "Epoch: [8/201] Batch: 240/241 Loss: 2.1881,\n",
      "Epoch: 9 LR: [0.001]\n",
      "Epoch: [9/201] Batch:  0/241 Loss: 1.6563,\n",
      "Epoch: [9/201] Batch: 30/241 Loss: 2.0392,\n",
      "Epoch: [9/201] Batch: 60/241 Loss: 1.6229,\n",
      "Epoch: [9/201] Batch: 90/241 Loss: 1.6953,\n",
      "Epoch: [9/201] Batch: 120/241 Loss: 1.9834,\n",
      "Epoch: [9/201] Batch: 150/241 Loss: 2.4296,\n",
      "Epoch: [9/201] Batch: 180/241 Loss: 2.0583,\n",
      "Epoch: [9/201] Batch: 210/241 Loss: 1.8485,\n",
      "Epoch: [9/201] Batch: 240/241 Loss: 2.0152,\n",
      "Epoch: 10 LR: [0.001]\n",
      "Epoch: [10/201] Batch:  0/241 Loss: 1.7560,\n",
      "Epoch: [10/201] Batch: 30/241 Loss: 1.5536,\n",
      "Epoch: [10/201] Batch: 60/241 Loss: 2.2727,\n",
      "Epoch: [10/201] Batch: 90/241 Loss: 1.7990,\n",
      "Epoch: [10/201] Batch: 120/241 Loss: 2.0094,\n",
      "Epoch: [10/201] Batch: 150/241 Loss: 1.4097,\n",
      "Epoch: [10/201] Batch: 180/241 Loss: 2.1295,\n",
      "Epoch: [10/201] Batch: 210/241 Loss: 2.3370,\n",
      "Epoch: [10/201] Batch: 240/241 Loss: 1.6191,\n",
      "Saving Checkpoint...\n",
      "Epoch: 11 LR: [0.001]\n",
      "Epoch: [11/201] Batch:  0/241 Loss: 1.7206,\n",
      "Epoch: [11/201] Batch: 30/241 Loss: 1.6460,\n",
      "Epoch: [11/201] Batch: 60/241 Loss: 1.8867,\n",
      "Epoch: [11/201] Batch: 90/241 Loss: 1.5895,\n",
      "Epoch: [11/201] Batch: 120/241 Loss: 1.6228,\n",
      "Epoch: [11/201] Batch: 150/241 Loss: 1.9209,\n",
      "Epoch: [11/201] Batch: 180/241 Loss: 2.0417,\n",
      "Epoch: [11/201] Batch: 210/241 Loss: 2.5717,\n",
      "Epoch: [11/201] Batch: 240/241 Loss: 2.0800,\n",
      "Epoch: 12 LR: [0.001]\n",
      "Epoch: [12/201] Batch:  0/241 Loss: 1.8274,\n",
      "Epoch: [12/201] Batch: 30/241 Loss: 2.7208,\n",
      "Epoch: [12/201] Batch: 60/241 Loss: 2.7132,\n",
      "Epoch: [12/201] Batch: 90/241 Loss: 1.2765,\n",
      "Epoch: [12/201] Batch: 120/241 Loss: 2.1398,\n",
      "Epoch: [12/201] Batch: 150/241 Loss: 2.3122,\n",
      "Epoch: [12/201] Batch: 180/241 Loss: 2.1921,\n",
      "Epoch: [12/201] Batch: 210/241 Loss: 2.3895,\n",
      "Epoch: [12/201] Batch: 240/241 Loss: 2.1785,\n",
      "Epoch: 13 LR: [0.001]\n",
      "Epoch: [13/201] Batch:  0/241 Loss: 1.9395,\n",
      "Epoch: [13/201] Batch: 30/241 Loss: 1.5172,\n",
      "Epoch: [13/201] Batch: 60/241 Loss: 1.6759,\n",
      "Epoch: [13/201] Batch: 90/241 Loss: 2.2053,\n",
      "Epoch: [13/201] Batch: 120/241 Loss: 1.7197,\n",
      "Epoch: [13/201] Batch: 150/241 Loss: 1.9654,\n",
      "Epoch: [13/201] Batch: 180/241 Loss: 2.5614,\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
